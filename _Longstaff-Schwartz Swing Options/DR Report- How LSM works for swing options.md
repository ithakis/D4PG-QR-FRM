Great, I’ll begin compiling a rigorous technical report on adapting the Longstaff–Schwartz least-squares Monte Carlo (LSM) method to price swing options, including mathematical formulation, algorithmic structure, basis function selection, and convergence diagnostics. The report will be written in Markdown format with equations and include relevant Python code snippets. I’ll also cite at least five peer-reviewed or industry sources to support the methodology.

I’ll let you know once the report is ready for review.


# Pricing Swing Options with Least-Squares Monte Carlo (LSM)

## Swing Options: Multiple Exercise Rights and Constraints

Swing options generalize American-style options by allowing **multiple early exercises** over the contract’s life. They are common in energy markets (e.g. natural gas or electricity take-or-pay contracts) where the holder can **“swing”** the delivery volume up or down each period within specified limits. In mathematical terms, a swing option grants \$R\_{\max}\$ total exercise rights (or a maximum total volume) across \$T\$ periods, subject to *local constraints* (e.g. at most \$r\_{\max}^{(t)}\$ units can be exercised in period \$t\$) and *global constraints* (e.g. a minimum total volume \$V\_{\min}\$ must be used by expiry, otherwise penalties apply). This turns the pricing into a **multiple stopping problem** under path-dependent volume limits. For example, Jaillet *et al.* (2004) model swing contracts where “subject to daily as well as periodic constraints, these contracts permit the holder to repeatedly exercise the right to receive greater or smaller amounts of energy”. Thus, the option’s state must include not only the underlying price \$S\_t\$ (and potentially other factors like fuel costs or interest rates) but also the remaining exercise rights \$r\$ and/or cumulative volume used so far. The option’s payoff depends on both price and exercised volume, and often there are penalties for unused minimum volume or for violating total usage constraints.

**Mathematical formulation:** Denote by \$V(t,r; S\_t)\$ the *value function* at time \$t\$ when the underlying price is \$S\_t\$ and \$r\$ exercise rights remain. Let \$\pi(S\_t, k)\$ be the immediate payoff from exercising \$k\$ units at price \$S\_t\$ (for example, \$\pi(S\_t,k) = k\max(S\_t - K,0)\$ for a swing call with strike \$K\$). At each exercise opportunity \$t\$, the holder can choose any \$0\le k \le \min{r,,r\_{\max}^{(t)}}\$ units to exercise (including \$k=0\$, i.e. waiting). Exercising \$k\$ rights yields immediate cashflow \$\pi(S\_t,k)\$ and leaves \$r-k\$ rights for the future. The *dynamic programming* (DP) recursion is:

$$
V(t,r; S_t) \;=\; \max_{0 \le k \le \min(r,\,r_{\max}^{(t)})} \Big\{ \pi(S_t, k) \;+\; \mathbb{E}\!\big[\,V(t+1,\,r-k;\; S_{t+1}) \mid S_t\,\big] \Big\},
$$

with terminal condition \$V(T, r; S\_T) = \pi(S\_T,,k^*\_T)\$ at expiry \$T\$ (using any remaining rights optimally at the last period, e.g. \$k^**T = \min{r,,r*{\max}^{(T)}}\$ units if \$\pi(S\_T,1)>0\$, or paying a penalty if a minimum usage \$V\_{\min}\$ was not met). This Bellman equation reflects that the optimal value comes from comparing exercising \$k\$ units now vs. saving them for later. The multiple exercise feature introduces **path-dependency**: the optimal decision at \$t\$ depends on \$r\$, which in turn depends on previous exercise actions. In particular, only a limited number of rights can be exercised per period and “the price depends also on the number of living rights (i.e., an American-Asian-style payoff)”. This makes swing valuation considerably more complex than standard single-exercise American options.

## LSM Method Overview for American Options

The Longstaff–Schwartz (2001) LSM algorithm provides a flexible simulation-based approach to price American options. The key idea is to use **least-squares regression** on Monte Carlo simulation paths to approximate the **continuation value**, i.e. the expected payoff from holding the option (continuing) rather than exercising at a given time. In each state, the algorithm compares the immediate exercise payoff to the estimated continuation value to decide an optimal exercise policy. Longstaff & Schwartz showed that this regression approach can handle path-dependent and multifactor situations where traditional lattice/PDE methods struggle. Essentially, LSM estimates the conditional expectation \$C(t, S\_t) = \mathbb{E}\[V(t+1,|, \text{continue}) \mid S\_t]\$ by projecting future payoffs on a basis of functions of \$S\_t\$. This yields an approximate exercise rule that, when applied to the simulated paths, gives a **biased low** estimate of the true option value (since suboptimal exercise may occur due to regression error). Despite the bias, the estimate converges from below as the number of basis functions and simulation paths increases.

For a standard American option with single exercise, LSM works backward from the final time. At maturity \$T\$, the value is simply \$V(T,S\_T) = \max{\text{payoff}(S\_T), 0}\$. At each earlier time \$t\<T\$, for each simulated path one computes the *realized* continuation payoff (the discounted value at \$t\$ of the option if not exercised at \$t\$ and following the optimal strategy thereafter). Then by regressing those continuation payoffs against functions of the state \$S\_t\$, one obtains \$\hat{C}(t,S\_t) \approx \mathbb{E}\[V(t+1)\mid S\_t]\$. The optimal decision is to exercise at \$t\$ if the immediate payoff exceeds this continuation value estimate. This approach “estimates the conditional expected payoff to the option holder from continuation” via least squares, and is widely used due to its simplicity and power.

To extend LSM to **swing options**, we must incorporate the *multiple* exercise decisions. Conceptually, we treat \$(S\_t, r)\$ as the state and apply a similar backward induction. However, now the continuation value depends on the remaining rights \$r\$ and possibly on how exercising now changes that state. In practice, this means at time \$t\$ we need estimates for **two (or more) continuation values**: (1) if we do *not* exercise (\$k=0\$) and continue with \$r\$ rights, and (2) if we exercise some amount (say \$k=1\$ unit) and continue with \$r-1\$ rights. In general if up to \$k\_{\max}\$ units can be exercised, one might evaluate \$k=0,1,\dots,k\_{\max}\$ and choose the best. The LSM framework can accommodate this by performing **multiple regressions** or a single multi-dimensional regression to approximate \$\mathbb{E}\[V(t+1, r)]\$ and \$\mathbb{E}\[V(t+1, r-k)]\$ as functions of the current state. Ibáñez (2004) introduced just such a simulation approach for securities with “many (buying or selling) rights, but for which a limited number can be exercised per period, and \[with] penalties if a minimum quantity is not exercised”. His method essentially extends the LSM algorithm (building on Ibáñez & Zapatero 2004) to handle the extra state variable by regression. The result is an estimated **optimal policy** for using the swing rights over time, and an associated price as the expected discounted payoff under that policy.

## Continuation Value Regression for Multiple Exercises

Under the DP formulation \$V(t,r)\$ above, the continuation value if we *do not* exercise at time \$t\$ is \$C\_{\text{no}}(t,r; S\_t) = \mathbb{E}\[,V(t+1,,r; S\_{t+1}) \mid S\_t,]\$. If we exercise one unit (for example) at \$t\$, the continuation value for the remaining \$r-1\$ rights is \$C\_{\text{ex}}(t,r; S\_t) = \mathbb{E}\[,V(t+1,,r-1; S\_{t+1}) \mid S\_t,]\$. In an LSM implementation, we **simulate a large number of price paths** \$S\_0, S\_1, \dots, S\_T\$ and carry out backward induction as follows:

* **At \$T\$:** For each path \$i\$, set \$V(T, r\_i; S\_T^{(i)}) = \pi(S\_T^{(i)},,k^*\_T)\$, where \$k^*\_T\$ is the optimal final exercise (e.g. use all remaining rights if payoff positive). In many cases, \$k^\**T = \min{r\_i,,r*{\max}^{(T)}}\$ if immediate payoff is positive (and if any minimum volume requirement is unmet, a penalty is applied now). For example, if at maturity you can use at most 1 unit, then \$V(T,r;S\_T) = \max{\pi(S\_T,1),,0}\$ for any \$r\ge 1\$.

* **At each earlier time \$t = T-1, T-2, \dots, 1\$:** For each path \$i\$ that still has \$r>0\$ rights remaining at \$t\`\$, we already have computed the *realized* future values \$V(t+1, r; S\_{t+1}^{(i)})\$ and \$V(t+1, r-1; S\_{t+1}^{(i)})\$ from the next time step (either from the initialization at \$T\$ or a previous backward step). We then **regress** those next-step values on functions of the current state. In practice, one can perform separate regressions for each \$r\$ (or include \$r\$ as an input variable in a single regression). For example, one regression could fit \$\hat{C}*{\text{no}}(t,r; S\_t) \approx \mathbb{E}\[V(t+1,r)\mid S\_t]\$ and another fit \$\hat{C}*{\text{ex}}(t,r; S\_t) \approx \mathbb{E}\[V(t+1,r-1)\mid S\_t]\$ using the simulated data. The basis functions for regression might include polynomials or other functions of \$S\_t\$ (and possibly of \$r\$ if done in one regression). This step yields an approximate **continuation value** for each possible action. Then, for each path, we compare the *immediate* payoff vs. *continuation*:

  * If exercising 1 unit now yields value \$\pi(S\_t^{(i)},1) + \hat{C}*{\text{ex}}(t,r; S\_t^{(i)})\$ greater than \$\hat{C}*{\text{no}}(t,r; S\_t^{(i)})\$ (the value of waiting), then it is optimal to exercise one unit on path \$i\$ at time \$t\$. In that case we set \$V(t,r; S\_t^{(i)}) = \pi(S\_t^{(i)},1) + \text{(discounted)},V(t+1, r-1; S\_{t+1}^{(i)})\$. If multiple units can be exercised, one could evaluate \$k=2\$ similarly or even solve an integer optimization if \$k\_{\max}>1\$. In many swing contracts, however, \$k\_{\max}=1\$ per period (one exercise per day, for instance), which simplifies the decision to a binary choice: exercise one vs. none.

  * If waiting is better, we set \$V(t,r; S\_t^{(i)}) = \text{(discounted)},V(t+1, r; S\_{t+1}^{(i)})\$ (carry the value forward with \$r\$ still intact).

By iterating this backward, we derive an *exercise strategy*: a mapping of states \$(S\_t, r)\$ to the decision (exercise how much or wait). Notably, **regression is used to estimate the conditional expectations** needed for these decisions. The LSM approach remains “the most commonly used method for computing the continuation value” in dynamic programs, approximating it via “successive orthogonal projection over subspaces spanned by a finite number of square-integrable basis functions”. Multiple studies have extended this approach to swing options with various refinements. For instance, Meinshausen & Hambly (2004) treated the swing valuation as a **reinforcement learning** problem, using *Q-learning* algorithms to learn the optimal exercise policy under both local and global constraints. Their pioneering work demonstrated that simulation-based methods can indeed handle the complex constraints of swing contracts by augmenting the state and optimizing decisions sequentially. Other authors have proposed alternative regression-based algorithms – e.g. with inequality-constrained least squares to enforce monotonicity of the value in \$r\$ – to improve numerical stability.

It is important to recognize that in a multi-exercise setting, the **exercise boundary** is more complex: it’s not just a single critical price curve, but a decision surface depending on how many rights remain. Intuitively, the fewer rights left, the closer the problem is to a regular American option (you become more selective with exercises as rights diminish). Conversely, with many rights remaining, the holder can be more aggressive in exercising on price spikes. In fact, optimal policies often exhibit a *bang-bang* nature: either use the maximum allowed or nothing, depending on whether price is above a certain trigger. For example, Barrera-Esteve *et al.* (2006) showed that for swing options with certain penalty structures, the optimal daily exercise is **digital** (all-or-nothing) – either take the full allowable quantity or skip, based on the price threshold. This aligns with the idea that the continuation value function \$V(t,r;S)\$ is generally **concave in the remaining rights** \$r\$, which tends to favor extreme decisions \$k=0\$ or \$k\_{\max}\$. The LSM regression at each step effectively tries to capture this concave value function in \$S\_t\$, for each discrete \$r\$.

## Basis Function Choice and Bias–Variance Tradeoff

A crucial modeling choice in LSM is the **basis functions** used to approximate continuation values. Longstaff & Schwartz (2001) originally used simple polynomials (up to degree 3) and found that results were fairly insensitive to the specific family chosen. In their tests, Laguerre polynomials (weighted polynomials orthogonal with respect to an exponential weight) worked well, but Hermite or Legendre polynomials or even simple power functions yielded similar option values. The primary goal is to capture the shape of the true conditional expectation \$C(s)\$; any complete functional family can do so given enough terms, but finite samples and multicollinearity can make some bases more efficient than others. For example, highly correlated basis functions can lead to unstable regression coefficients (the **multicollinearity** problem). Orthogonal polynomial families like Laguerre or Hermite mitigate this by producing nearly independent regressors and focusing on different regions of the state space. Laguerre polynomials in particular are orthogonal with respect to an exponential weight, which is convenient for processes that have a natural decay in the state distribution (like positive stock prices under certain measures). They also have recursion formulas that enable stable computation of higher-degree terms. Chebyshev polynomials are another alternative, offering near-minimax approximation error, but they require mapping the domain of \$S\_t\$ to $\[-1,1]\` which can be inconvenient for unbounded domains.

Despite Longstaff & Schwartz’s observation of minimal differences, later researchers did find some basis choices outperform others in specific cases. For instance, in one empirical study comparing 11 polynomial families for LSM, **weighted Laguerre polynomials** provided the most accurate results for complex options (like compound or multiple-exercise options) when using the same number of basis functions. The intuition is that a good basis can approximate the continuation payoff with fewer terms, reducing **bias** without incurring too much **variance** from overfitting. With too few basis functions, the regression will be biased (underestimating continuation), leading to suboptimal exercise decisions and a low-biased option value. Using more basis functions can reduce approximation bias but at the cost of higher variance in the estimated coefficients (especially if the Monte Carlo sample size is fixed). This variance manifests as noise in the exercise boundary and can actually lower the estimated option value if it leads to some mistaken exercises or non-exercises. The optimal complexity balances these two. In practice, one can increase the basis size until the estimated price stabilizes (within simulation error), or use cross-validation to pick a basis that best predicts out-of-sample continuation values. For swing options with high-dimensional state (price plus remaining rights, etc.), one might also use basis functions that capture interactions (e.g. polynomials in both \$S\_t\$ and \$r\$) or even non-polynomial approximators. **B-spline bases** and other piecewise polynomials have been used to approximate the value function in dynamic programming for swing contracts, yielding flexible fits that can adapt to kinks in the value function (which often occur at exercise boundaries). Recently, researchers have also explored machine learning approaches (like neural networks) to approximate the continuation value or policy directly, which can be seen as using a very rich function class for the basis. While such approaches can reduce bias dramatically, one must be careful to avoid overfitting the Monte Carlo noise – regularization or very large sample sizes may be needed to get reliable results.

In summary, **basis selection** affects the bias–variance tradeoff of LSM estimates. Using polynomials of too low degree can bias the price significantly low (missing nonlinear features of \$C(s)\$), whereas a very large polynomial basis can overfit noise (producing erratic exercise decisions and large statistical errors). Orthogonal or smooth basis functions (Laguerre, Hermite, splines, etc.) often improve numerical stability and convergence. Ultimately, there is no one-size-fits-all: the choice may depend on the option’s payoff structure. For swing options in particular, basis functions might need to capture the dependence on remaining rights – one approach is to perform separate regressions for each \$r\$, effectively using a different set of basis functions for each sub-problem. This partitions the data by \$r\$ (reducing sample size per regression), but ensures the functional form is tailored to each rights count. Another approach is to include \$r\$ as an input and perhaps use *piecewise* basis functions in \$r\$ (since \$r\$ is discrete and bounded). For example, we could include indicator basis functions for specific \$r\$ values, or treat \$r\$ as continuous and use polynomials in \$r\$ (less accurate unless \$r\$ is large). The **optimal regression design** for multiple stopping remains an area of research; some works combine regression with **stochastic mesh** or **recursive partitioning** to better learn the value function across a multi-dimensional state space. Nonetheless, polynomial bases of moderate degree have proven effective in many practical implementations of swing option LSM pricing.

## Monte Carlo Simulation Workflow

Implementing LSM for swing options involves the following computational steps:

**1. Path Generation:** Simulate a large number \$N\$ of price paths under the risk-neutral process for the underlying asset (or multiple risk factors). For energy commodities, the model could be a mean-reverting process with seasonality and jumps, or a multi-factor forward curve model. For simplicity, one might use a Geometric Brownian Motion for underlying spot \$S\_t\$ or simulate directly the forward price dynamics for each delivery period. Each path will be a sequence \${S\_0^{(i)}, S\_1^{(i)}, \dots, S\_T^{(i)}}\$ of prices. Also track any relevant state variables like remaining rights \$r\$ or cumulative volume. If decisions do not affect the underlying price process (usually they don’t, as we assume the holder is non-market-moving), we can simulate all paths upfront. Typically \$N\$ needs to be in the thousands or more for accurate results (depending on payoff complexity and basis dimension).

**2. Backward Induction (Regression and Optimal Decisions):** Start at the final exercise time and move backwards:

* At \$t=T\$: For each path \$i\$, compute the payoff if exercising optimally all remaining rights. This sets \$V(T,r^{(i)}*T; S\_T^{(i)})\$. In code, this could be something like `V_T[i] = payoff(S_T[i], k = min(r[i], r_max_T))`. For example, if only one unit can be exercised at \$T\$, then \$V\_T^{(i)} = \max{\pi(S\_T^{(i)},1),,0}\$ if \$r^{(i)}>0\$, or 0 if \$r^{(i)}=0\$. Apply any penalty for unused minimum volume here as well (which would reduce \$V\_T\$ for paths that didn’t meet \$V*{\min}\$).

* For each earlier time \$t = T-1, T-2, ..., 0\$: we perform the regression routine described above. In a typical implementation, one would loop over each possible \$r\$ value (from \$0\$ up to \$R\_{\max}\$) and handle those paths that have that many rights remaining at time \$t\$. However, since at time 0 all rights are unused (e.g. \$r=R\_{\max}\$), one can simplify by iterating \$r\$ downwards as we go back in time. On iteration \$t\$, for each path \$i\$ (that is still “alive”, meaning it hasn’t exhausted rights by a prior optimal exercise):

  * Compute the *discounted* continuation values for the next step: e.g. \$Y\_{\text{no},i} = \exp(-\Delta t , r\_f),V(t+1, r^{(i)}; S\_{t+1}^{(i)})\$ (if no exercise now) and \$Y\_{\text{ex},i} = \exp(-\Delta t , r\_f),V(t+1, r^{(i)}-1; S\_{t+1}^{(i)})\$ (if exercise one now, assuming \$r^{(i)}>0\$). Here \$r\_f\$ is the risk-free rate for discounting one time step \$\Delta t\$. These \$Y\$ values are known from the already computed future payoffs.
  * Use least-squares to fit \$Y\_{\text{no}}\$ as a function of the current state. A simple regression might use a basis of \$\phi\_j(S\_t)\$ functions (like \$1, S\_t, S\_t^2\$) to get \$\hat{C}*{\text{no}}(S\_t) = \sum\_j \beta\_j \phi\_j(S\_t) \approx \mathbb{E}\[Y*{\text{no}} \mid S\_t]\$. Similarly, fit \$Y\_{\text{ex}}\$ to get \$\hat{C}*{\text{ex}}(S\_t) \approx \mathbb{E}\[Y*{\text{ex}} \mid S\_t]\$. This can be done with ordinary least squares across all simulated paths (or the subset in-the-money; often one restricts regression to paths where exercise is relevant, e.g. where \$\pi(S\_t,1)>0\$, to avoid poor extrapolation).
  * For each path \$i\$, compare \$\pi(S\_t^{(i)},1) + \hat{C}*{\text{ex}}(S\_t^{(i)})\$ to \$\hat{C}*{\text{no}}(S\_t^{(i)})\$. If the former is larger, set path \$i\$ to exercise at \$t\$ (meaning we record an exercise and decrement \$r^{(i)}\$ by 1). The value along that path at \$t\$ becomes \$V(t,r^{(i)}; S\_t^{(i)}) = \pi(S\_t^{(i)},1) + \exp(-\Delta t,r\_f),V(t+1, r^{(i)}-1; S\_{t+1}^{(i)})\$. If instead waiting is better, we set \$V(t,r^{(i)}; S\_t^{(i)}) = \exp(-\Delta t,r\_f),V(t+1, r^{(i)}; S\_{t+1}^{(i)})\$. (If \$r^{(i)}=0\$, no decision is possible – the path simply carries \$V(t,0)=\exp(-\Delta t,r\_f),V(t+1,0)\$ forward.)
  * These computed \$V(t,\cdot)\$ values will be used in the regression one step earlier. Note: If an exercise happens on path \$i\$ at time \$t\$, then for all \$\tau < t\$ that path will use the \$r^{(i)}\$ decreased by one when considering future values. In effect, after deciding exercises at \$t\$, you would mark that path’s later payoffs accordingly (which we already did by using the \$V(t+1, r-1)\$ in that case).

This backward induction yields the *optimized cashflows* for each path: each path \$i\$ now carries a sequence of realized payoffs (some at certain exercise times, possibly zero at others). The **LSM price estimator** is then the average of the discounted payoffs across all paths: \$P\_0 \approx \frac{1}{N}\sum\_{i=1}^N \sum\_{t=0}^T \text{cashflow}\_{t}^{(i)} \exp(-r\_f,t\Delta t)\$, which is an unbiased estimator of the lower bound (given the policy came from the regression). Often, one can reuse the *same paths* used for training to estimate this expected payoff (this gives a slightly biased lower bound because the policy was optimized in-sample, but with a large \$N\$ the bias is negligible). Alternatively, to be safer, one can simulate a fresh set of paths following the derived exercise strategy to get the average payoff (this removes any upward bias from overfitting, at the cost of a slightly lower value if the policy was imperfect).

Below is a **pseudo-code example** (Python-style) demonstrating a simplified LSM for a swing call option with at most 2 exercise rights over 2 periods. We use polynomial regression to estimate continuation values. (For brevity, we assume at most 1 unit can be exercised per period and no minimum usage penalty in this example.)

```python
import numpy as np

# Example parameters
S0 = 100; K = 100          # initial price and strike
vol = 0.2; r = 0.0         # volatility and risk-free rate
T = 2; dt = 0.5            # 2 exercise periods, 6 months apart
N = 10000                  # number of simulated paths
np.random.seed(42)

# 1. Simulate GBM paths for S1 and S2
Z = np.random.normal(size=(N, T))
S = np.zeros((N, T+1)); S[:,0] = S0
for t in range(1, T+1):
    S[:,t] = S[:,t-1] * np.exp((r - 0.5*vol**2)*dt + vol*np.sqrt(dt)*Z[:,t-1])

# 2. Backward induction
# Terminal payoff at t=2 for each path (if at least 1 right remains)
payoff_T = np.maximum(S[:,T] - K, 0)  # one-unit exercise payoff
# At maturity, if R>=1, exercise one if positive payoff:
V_T_r1 = payoff_T  # value if 1 right remains
V_T_r2 = payoff_T  # value if >=1 rights remain (only one can be used)
V_T_r0 = np.zeros(N)

# At t=1 (first exercise time):
# Prepare regression targets: continuation values if skip or exercise
disc = np.exp(-r*dt)
Y_no  = disc * V_T_r2    # if no exercise at t=1, (r=2 remains to t=2)
Y_ex  = disc * V_T_r1    # if exercise 1 at t=1, (r=1 remains to t=2)
X = np.column_stack([np.ones(N), S[:,1], S[:,1]**2])  # basis (1, S, S^2)
# Least-squares regression
beta_no = np.linalg.lstsq(X, Y_no, rcond=None)[0]
beta_ex = np.linalg.lstsq(X, Y_ex, rcond=None)[0]
C_no = X.dot(beta_no)    # estimated E[V_T | no exercise]
C_ex = X.dot(beta_ex)    # estimated E[V_T | exercise one]

# Determine optimal exercise at t=1 for each path
exercise = (np.maximum(S[:,1]-K, 0) + C_ex > C_no)
# Compute value at t=1 given the decision
V_1 = np.where(exercise,              # if exercised:
               np.maximum(S[:,1]-K,0) + disc*V_T_r1,   # immediate + cont (r1)
               disc*V_T_r2 )                          # if not exercised (r2)

# At t=0 (pricing time):
price_est = np.exp(-r*dt) * V_1.mean()   # discount t=1 values back to t=0
print(f"Estimated swing option price: {price_est:.3f}")
```

In this code, we simulate price paths for two periods and perform a simplified backward induction with polynomial regression. We find the exercise rule at \$t=1\$ and then estimate the price as the discounted average payoff. (A real implementation would handle the state variable \$r\$ more generally and possibly iterate a loop for each \$t\$.) The above might output, for example, an estimated price around 13.6 for this at-the-money 2-exercise call scenario. Of course, this is a toy illustration – in practice, one would use more paths, higher-degree polynomials or other basis functions, and include robustness checks.

**Variance reduction:** Monte Carlo error can be significant, so variance reduction techniques are often employed. **Antithetic variates** (using paired paths with negated random shocks) can reduce variance in the simulation of underlying prices. One can also use **control variates**, for example pricing a simpler derivative (like a strip of European options or a single American option) analytically or with high precision and using it to adjust the swing estimate. For instance, a swing with \$R\_{\max}\$ rights lies between an American option and a forward contract on \$R\_{\max}\$ units; these bounds can serve as controls. Stratified sampling or low-discrepancy Sobol sequences may also reduce variance in high dimensions. Another useful trick is **common random numbers** in regression: use the same set of random paths to evaluate both \$\hat{C}*{\text{no}}\$ and \$\hat{C}*{\text{ex}}\$ so that their estimation errors cancel out in the comparison (this is naturally done when using one set of paths for both, as above). In more advanced implementations, one might even use **multi-level Monte Carlo** for swing options, simulating a hierarchy of path discretizations to reduce variance efficiently.

**Parallelization:** The LSM algorithm is embarrassingly parallel in many parts. Path generation can be parallelized across multiple CPU cores or GPUs – each core can simulate a subset of paths. The regression at each time step involves operations on large matrices of dimension \$N \times m\$ (with \$m\$ basis functions), which can be accelerated with linear algebra libraries or GPUs. In fact, one can distribute the paths across machines and aggregate the sufficient statistics for regression (like \$X^TX\$ and \$X^TY\$) to perform a globally optimal regression without needing all data on one machine. The backward time steps must be done sequentially (due to dependency), but within each time step the computations are vectorized and parallel. Modern implementations often leverage GPU to simulate millions of paths and perform regressions with hundreds of basis functions in reasonable time. For example, in the code above, if \$N\$ were very large, one could use GPU libraries to perform the simulation loop and the `np.linalg.lstsq` solve much faster. Additionally, **reinforcement learning** methods mentioned earlier can be parallelized by running many simulated experiences in parallel and using stochastic gradient descent to update the value function approximation. Overall, scalability is one of LSM’s strengths – it trades off easily with more compute power to handle the curse of dimensionality (whereas tree or finite-difference methods struggle beyond 2–3 dimensions). This makes LSM (and its deep learning variants) attractive for complex swing contracts with many factors and constraints.

## Convergence Diagnostics and Error Estimation

Because LSM yields a **lower-bound** estimate of the true option value (due to potential suboptimal exercise decisions from approximation error), it is important to assess the quality of the result. Several diagnostics and techniques are used:

* **Increasing Simulation Effort:** One basic check is to increase the number of paths \$N\$ and/or the polynomial degree \$m\$ to see if the estimated price converges. If the price changes significantly when going from, say, 50,000 to 100,000 paths or when adding an extra basis function, it indicates the estimate is not yet stable. In a well-behaved case, the estimate will approach a plateau as \$N\to\infty\$ (statistical error \$\sim O(1/\sqrt{N})\$) and as \$m\$ becomes large enough to capture the continuation function. However, Stentoft (2004) noted that **LSM may fail to converge** to the true price if the basis functions are inadequate to represent the optimal stopping boundary. Thus, one should try different functional forms and ensure that the chosen basis can approximate the expected payoff function over the relevant range of \$S\_t\$. Plotting the fitted continuation value vs. \$S\_t\$ (for various \$r\$) and the empirical regression \$R^2\$ can be informative – a very low \$R^2\$ might mean the basis is misspecified or noise is high.

* **Out-of-Sample Validation:** A prudent practice is to split the simulation paths into a training set (for regression) and a test set. Use the training set to derive the exercise policy, then **simulate the policy on the test set** to estimate the price. This avoids upward bias from overfitting. If the training-based and out-of-sample price estimates differ, the difference gives an indication of the approximation error. Large discrepancies mean the regression might be overfitting or the policy is not truly optimal for unseen paths. In multiple exercise problems, one can also validate structural properties – e.g., the optimal exercise boundary in theory should be monotonic in \$r\$ (the more rights left, the lower the exercise threshold, since you can afford to wait less). If the learned policy violates such properties (e.g., recommends exercising with 2 rights in some state but not with 1 right in the same price state, which would be inconsistent), it may signal regression noise. Techniques like **constrained regression** (enforcing monotonicity in \$r\$) can be applied to ensure reasonable policies.

* **Duality-based Error Bounds:** One of the most powerful techniques to evaluate American option pricing error is the **primal-dual approach**. Given a proposed exercise policy (primal solution), one can compute an upper bound on the true price by finding a **martingale** (dual solution) that replicates the option payoff with some error. Rogers (2002) and Haugh & Kogan (2004) introduced this for American options, showing that for any feasible martingale \$M\_t\$ one has \$V\_0 \le \mathbb{E}\[\max\_{\tau}(X\_\tau + M\_0 - M\_\tau)]\$, and the tightest bound comes from an optimal martingale linked to the continuation values. Andersen & Broadie (2004) refined this into a practical algorithm to compute an upper confidence interval for the price. For swing options (multiple stopping), the dual formulation is more involved – essentially a linear programming or optimal control problem – but it has been **extended in the literature**. Notably, Barrera-Esteve *et al.* (2006) and others devised dual upper bounds for multi-exercise contracts by considering a family of martingales associated with each exercise right. Bender & Schoenmakers (2006) and later Bender (2011) developed a **stochastic dual dynamic programming** approach that provides both lower and upper bounds for multiple stopping problems. In practice, implementing the dual for swing options can be complex, but even a suboptimal dual can give a useful benchmark. If the duality gap (difference between dual upper bound and LSM lower bound) is small – say a few cents – one can be confident the LSM price is accurate. If the gap is large, one might refine the basis or increase simulation effort. In our context, suppose the LSM yields \$13.60 and a dual method gives an upper bound of \$13.85; then the true price lies in that interval, and the error is at most \$0.25. Some researchers also iterate: use the dual’s information to improve the basis functions adaptively (for example, adding basis functions that approximate the martingale correction).

* **Hedging performance:** As an indirect check, one can test how well the option can be hedged under the LSM policy. If one simulates market paths and implements a delta-hedging strategy assuming the LSM exercise boundary, the **hedging P\&L** should be near zero-mean if the pricing is accurate (in a complete market). Large systematic hedging losses/gains might indicate the exercise strategy is suboptimal or the price off. This is more relevant for physical commodity swing options where hedging might be done with futures; any arbitrage or significant residual risk could imply mispricing.

* **Convergence rate measurements:** For Monte Carlo error, one can compute the standard error of the mean payoff. For example, run \$N\$ paths in batches and see the sample variance of the batch means. This gives confidence intervals for the **statistical error** (though not accounting for bias). Typically, the half-width of the 95% CI shrinks as \$\sim 1.96,\sigma/\sqrt{N}\$. If that is, say, \$0.1, it means pure randomness is not a big factor compared to other errors. One can also perform **incremental convergence tests**: increase \$N\$ until the price change is within tolerance and increase basis until price change is within tolerance.

In conclusion, adapting LSM to swing options provides a flexible and powerful pricing method. It rigorously handles the **multiple stopping** nature by expanding the state (adding the remaining rights or volume as a state variable) and performing **continuation value regressions** for each possible action. Success relies on a careful choice of basis functions to keep bias low without exploding variance, as well as sufficient simulation trials to tame statistical noise. The computational workflow involves simulating paths, backward regressing, and making **greedy optimal decisions** at each step – a process that is amenable to parallelization and variance-reduction enhancements. Empirical studies (e.g. Meinshausen & Hambly 2004, Ibáñez 2004, etc.) have validated that LSM can price swing contracts under realistic assumptions. Finally, convergence diagnostics like dual bounds and out-of-sample tests help ensure that the reported price is reliable and quantify any residual error. With these techniques, practitioners can rigorously price complex swing derivatives and derive near-optimal exercise policies for risk management in energy markets.

**Sources:** Longstaff & Schwartz (2001); Ibáñez (2004); Meinshausen & Hambly (2004); Barrera-Esteve *et al.* (2006); Jaillet, Ronn & Tompaidis (2004); Rogers (2002); Haugh & Kogan (2004); Bender (2011).
